# Batmobile: Car Price Prediction
[![SkillFactory Data Science](https://img.shields.io/badge/SF-Data%20Science-brightgreen)](https://skillfactory.ru/data-science)
[![Kaggle](https://img.shields.io/badge/-Kaggle-34b6ef?logo=Kaggle&logoColor=white)](https://www.kaggle.com/c/sf-dst-car-price-prediction-part2)\
![TF](https://img.shields.io/badge/-TensorFlow-FF6F00?logo=TensorFlow&logoColor=white)
![NLP](https://img.shields.io/badge/Deep_Learning-NLP-1f8280)

Этот [финальный проект](https://www.kaggle.com/c/sf-dst-car-price-prediction-part2) курса _Real Data Science_ подводил итоги обучения по курсам _Machine Learning_ и _Deep Learning_ и являлся продолжением соревнования ([Project 3. Car Price Prediction](https://github.com/macsunmood/SkillFactory_RDS/tree/master/Project%203.%20Car%20Price%20Prediction)), но уже с дополнительными данными (текст и изображения).

Задача заключалась в доведении работы по созданию модели, предсказывающей стоимость автомобиля, до логического завершения: обогатить датасет текстовыми данными из объявлений о продаже и свести все модели в единое решение — **Multiple Inputs сеть**.

> Эффективно работающая, такая модель позволит быстро выявлять выгодные предложения, что **значительно ускорит работу менеджеров и повысит прибыль компании**.

## Описание и ход работы

Задача — предсказать стоимость автомобиля по его характеристикам, текстовому описанию и картинке.

### Целевая метрика:

- **[MAPE](http://en.wikipedia.org/wiki/Mean_absolute_percentage_error)** (mean absolute percentage error) — **средняя абсолютная ошибка в процентах**:

![MAPE](https://render.githubusercontent.com/render/math?math=MAPE%20=%20\dfrac{100}{n}%20\sum_{t=1}^{n}%20\frac{|Yt%20-%20\hat{Y}_t|}{Yt})

```
Где:
Yt – фактическое значение за анализируемый период;
Ŷt — значение прогнозной модели за анализируемый период;
n — количество периодов.
```

Предоставлялся готовый датасет с изображениями, табличными данными и текстовыми описаниями, а также baseline с опорным решением.\
Было разрешено использовать любые ML и DL алгоритмы и библиотеки.

### Основные этапы:

<details>
  <summary><b>Разработка решения:</b></summary>

  1. _Построение «наивной» ML-модели на табличных данных._\
  Для начала была построена самая простая модель, которая предсказывала среднюю цену по модели автомобиля и году его выпуска. С ней далее сравнивались другие модели.

  2. _Первичная обработка и нормирование признаков. **Exploratory Data Analysis**._\
  На этом этапе был проведён **EDA**, что выявило ненормальное распределение числовых признаков. 
  Также обнаружено, что в основном цены имеют значение, кратное 1000, поэтому решено пробовать округлять предсказания.
  
  3. _Data Preprocessing._\
  Проведена предобработка данных: удалено лишнее, заполнены пропуски, выполнена нормализация. Сделан парсинг некоторых категориальных признаков с извлечением числовых значений из текста. Ключевые слова из описания были выделены в отдельные признаки. Для приведения данных к распределению, близкому к нормальному, от некоторых числовых признаков перед нормализацией был взят логарифм. В конце произведены **Label Encoding** и **One-Hot Encoding** и добавлено **предсказание NaN-значений** признака `"Владение"` с помощью CatBoostRegressor.
  
  4. _Построение первой модели на основе градиентного бустинга с помощью **CatBoost**._\
  Испробована модель на основе CatBoostRegressor.
  
  5. _Решение задачи с помощью **DL** (модель на Tabular данных) и сравнение результатов._\
  Была построена простая нейронная сеть (Dense-модель) из трёх слоёв с дропаутами.

  6. _Добавление текстовых данных (**NLP**) и организация **Multi-Input** нейронной сети (табличные данные + текст)._\
  Сделана предобработка текста: очистка и лемматизация, убраны стоп-слова. Далее произведена векторизация текста и построена **LSTM-сеть** для его обработки. 
  Добавлена ещё одна сеть — созданная ранее для табличных данных.
  Затем их выходы были объединены в **Multi-Input сеть** для обработки и табличных данных, и текста одновременно.

  7. _Добавление обработки изображений в Multi-Input нейронную сеть._\
  Задействована сверточная сеть для анализа изображений с помощью **EfficientNet** (перенос обучения **EfficientNetB6**).
  Поскольку датасет небольшой, была сделана аугментация изображений с помощью библиотеки **albumentations**.
  Для подачи данных при обучении keras-модели использован итератор **tf.data.Dataset**. 
  Multi-Input сеть: добавлена обработка изображений и объединены выходы трёх нейросетей (табличные данные + текст + изображения). 
  Далее произведён **Fine-tuning** сети.
  
  8. _**Ансамблирование** градиентного бустинга и нейронной сети._\
  На этом этапе сделан **blending** с усреднением показаний сетей.
  
</details>

## Результаты

Занято **2-е место** ([macsunmood](https://www.kaggle.com/c/sf-dst-car-price-prediction-part2/leaderboard)) на момент проведения соревнования.\
Достигнутый показатель **MAPE** на kaggle: **`10.81432%`**  (улучшение на **`1.89297%`** к baseline: `12.70729%`)

---

### Что сделано - особенности решения:
- Датасет очищен от дубликатов, пропущенных значений, обработаны выбросы;
- Выполнена нормализация признаков, применены методы для кодирования категориальных признаков;
- Добавлена **l2-регуляризация**;
- Использован **LearningRateScheduler** для плавного уменьшения Learning rate;
- В NLP сделана **лемматизация**, удаление **стоп-слов**, испробованы **Bag of Words** и **TF-IDF**;
- Применены как стандартные так и и **SOTA-архитектуры** для обработки изображений;
- Добавлена **аугментация** данных;
- Использованы **Transfer-learning** и **Fine-tuning**.

### Итоги и выводы:
- В некоторых случаях хорошо сработал оптимизатор **Adamax**, в других - **Adam** с параметром `amsgrad=True`;
- Была подобрана оптимальная Learning Rate для каждого случая; использована техника управления LR LearningRateScheduler там, где это дает пользу;
- Хорошо себя показала архитектура **EfficientNetB6** с техникой **Fine-tuning**;
- NLP проработка и адаптация текста (лемматизация, bag of words, tfidf, etc.) не дали в этом кейсе существенного прироста score;
- Работа с таргетом - логарифмизация, нормализация - не дала улучшения результата, а только значительно ухудшала его на нейросетях.

<details>
  <summary><b>Возможные способы улучшения результата:</b></summary>
  <ul>
  <li>Ансамблирование предобученных нейросетей;
  <li>Динамическое увеличение размера картинки при Fine-tuning;
  <li>Другие стратегии разморозки слоев;
  <li>Cyclic Learning Rate;
  <li>Использование внешних датасетов для дообучения модели.
  <li>Появились идеи по совмещению NLP tokenization sequences с аугментированными через TF-IDF табличными данными, наверняка был был прирост при удачно подобранном vocabulary.
  </ul>
</details>
